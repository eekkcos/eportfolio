<!-- ind1.html -->
<div class="accordion-container">

  <!-- Task 1 -->
  <div class="accordion-item">
    <button class="accordion-button">Task 1: Datalake Ingestion for OCBCSG (Operations Department)</button>
    <div class="accordion-content">
      <div class="ind1text">
        <h3>Task 1: Datalake Ingestion for OCBCSG (Operations Department)</h3>
         <p>
            I was assigned the CLO (Consumer Loans Operation) Tier 3 Dashboard, which helps fellow OCBC colleagues in the CLO department to track their cases and KPI easily, improving their productivity and efficiency.
            As part of the data ingestion process we began by gathering user requirements through stakeholder meetings. 
            This helped us identify the essential columns needed and clarified which fields contained PII (Personally Identifiable Information). 
            After receiving the sample data, we anonymized all PII using hashing techniques and converted the files to .txt format for ingestion to UAT deployment.
          </p>
        <p>
          The ingestion pipeline followed: Landing → Staging → SRCDL via MobaXTerm.
        </p>

        <p>
          This phase was unexpectedly time-consuming due to my unfamiliarity with MobaXTerm’s environment and command structure. 
          Setting up sessions, managing credentials, and navigating the correct file paths posed significant challenges.
          We encountered permission-related issues when placing files into landing, often due to ownership settings. We resolved these by learning and applying vim and basic Linux permission commands. 
          Additionally, we manually removed headers before ingestion to meet staging format requirements.In staging, we faced frequent errors related to data partitioning. 
          If audit files weren't deleted from the correct paths in HUE, the ingestion failed.
        </p>


        <div class="ind1picture">
          <img src="public/mobaxterm.jpg" alt="MobaXTerm screenshot">
        </div>

        <p>Essential Vim/Linux commands:</p>
        <div class="code-box">
          <code>
            :wq! - save changes  
            :q! - quit  
            :w! - save without quitting  
            pwd - current directory  

            ls -lrt  
            -l long listing  
            -r reverse order  
            -t sort by modification time  

            vi file.txt - create/edit file  

            cat file.txt - view contents  
            cat file1.txt file2.txt > combined.txt - combine files  

            grep -i 1011 file1.txt - search '1011' case-insensitive  

            chown {user}:{group} file.txt - change ownership  
            chmod 775 file.txt/* - change permissions  

            I - insert  
            ESC - escape  

            wc -l file.txt - line count  
            CTRL+G - jump to bottom
          </code>
        </div>

        <p>
          I have learnt the importance of running MSCK REPAIR TABLE to sync Hive table metadata with actual partitions, initiate new dashboards using cleaned data from SRCDL, 
          being able to lay the foundation for future analytics projects.
        </p>
        <p>
          This project deepened my technical knowledge in HDFS, HUE, and Hive, and gave me hands-on experience with tools like MobaXTerm, Vim, and Linux shell commands. 
          I developed skills in troubleshooting ingestion issues, collaborating across teams, 
          and ensuring metadata consistency through commands like `MSCK REPAIR TABLE`, which helps to synchronize with the new data partition.
        </p>
        <p>
          At the end of this task, we successfully ingested multiple datasets from different source systems into SRCDL. 
          I independently handled permission errors, formatted staging files, and validated ingestion through Hive queries and HUE partition checks. 
          This laid the foundation for my future analytics projects, improving my troubleshooting and teamwork skills as well, through close collaboration with others.
        </p>
      </div>
    </div>
  </div>

  <!-- Task 2 -->
  <div class="accordion-item">
    <button class="accordion-button">Task 2: Development of Visualisation Dashboard for OCBCSG (Operations Department)</button>
    <div class="accordion-content">
      <div class="ind1text">
        <h3>Task 2: Development of Visualisation Dashboard for OCBCSG (Operations Department)</h3>

        <p>In the same project CLO Tier 3 Dashboard, the user initially provided us with a mockup dashboard of how they wanted to view and track different case metrics — such as the number of pending cases, today's statistics, and historical statistics.
            To support their requirements, we started by creating a Hive SQL table view that aggregated relevant operational data.
        </p>

        <h4>View Creation Logic:</h4>
        <p>
          This involved crafting complex joins across multiple source tables and applying case logic and filtering based on business rules (e.g.disbursement case logic for disbursement dashboard). 
          The logic we used is a nested SQL Query containing 3 layers:
        </p>
        <ul>
          <li><strong>Layer 1:</strong> Select required columns and pre-filter (e.g., disbursement status).</li>
          <li><strong>Layer 2:</strong> Join supplementary tables (calendar, ops mapping) to enrich data.</li>
          <li><strong>Layer 3:</strong> Final selection & transformation for visualization (SLA flags, case status labels).</li>
        </ul>

        <div class="ind1picture">
          <img src="public/view.png" alt="SQL View Diagram">
        </div>

        <p>
        Throughout the project, we conducted weekly user engagement calls where we presented dashboard iterations and clarified business logic. 
        These sessions helped us understand the users' business logic and visualization requirements more clearly.
        The dashboard was built using Power BI, where we connected to our database to get the previously ingested data to have a draft for the dashboards to do sanity checks.
        </p>
        <div class="ind1picture">
          <h4>Sanity Check:</h4>
          <img src="public/sanitycheck.jpg" alt="Sanity Check Screenshot">
        </div>

        <p>
        Throughout the project, we conducted weekly user engagement calls where we presented dashboard iterations and clarified business logic. 
        These sessions helped us understand the users' business logic and visualization requirements more clearly.
        The dashboard was built using Power BI, where we connected to our database to get the previously ingested data to have a draft for the dashboards to do sanity checks.
        </p>

        <div class="ind1picture">
          <h4>These are some <u>fill in</u> visuals to show how the dashboard has improved through weekly user engagement calls:</h4>
          <p>First mockup:</p>
          <img src="public/1.png" alt="Mockup 1">
          <p>Updates after calls:</p>
          <img src="public/2.png" alt="Mockup 2">
          <p>Final Layout:</p>
          <img src="public/3.png" alt="Final Dashboard">
        </div>

        <p>
          The finalized Power BI dashboard is now used by the CLO Department to track disbursement cases, 
          making sure they are all disbursed by the end of the day.
          Iterative user feedback helped us align business definitions and logic, ensuring the dashboard met operational and audit requirements.
        </p>

      </div>
    </div>
  </div>

  <!-- Task 3 -->
  <div class="accordion-item">
    <button class="accordion-button">Task 3: DevOps Framework</button>
    <div class="accordion-content">
      <div class="ind1text">
        <h3>Task 3: DevOps Framework</h3>
        <p>I was introduced to the DevOps Framework adopted by the team. 
          As an intern, I could not participate in the deployment/automation processes, 
          but I knew it was important to understand and I gained a deeper understanding of tools like JIRA, BitBucket, Confluence and Jenkins are integrated into our developement and release cycle.
        </p>
        <p>Everytime a project is started, departments follow the DevOps workflow, but some choose to customise, fitting to their own needs accordingly.

        </p>


        <h5>Confluence:</h5>
        <p>Centralized documentation with version control.</p>

        <h5>JIRA:</h5>
        <p>Ticketing system with nodes: Story, Task, Epic, Bug, SIT Release, UAT/PROD Release.</p>

        <div class="ind1picture">
          <img src="public/JIRA.jpg" alt="JIRA Screenshot">
          <img src="public/devops2.jpg" alt="DevOps Screenshot">
        </div>

        <h5>Bitbucket & Jenkins:</h5>
        <p>Bitbucket stores code; Jenkins automates builds and debugging. Pull Requests are used for code reviews.</p>

        <div class="ind1picture">
          <h5>Infinity Ticket:</h5>
          <img src="public/devops1.jpg" alt="Infinity Ticket">
        </div>

        <h5>Change Activity Committee (CAC):</h5> 
          <p>Reviews all upcoming deployments/changes</p>
          <ul>
            <li>Details from Infinity Ticket are compiled into an Excel sheet and discussed</p>
            <li>Does the request follow the policy? Are all approvals and testing done? Are Security Requirements cleared?</p>
            <li>If approved, deployment can proceed after office hours (Green Zone), or during office hours (Red Zone; More sensitive)</p>
          </ul>

        <p>I didn’t know there were so many steps and checks before something can be deployed.
            This experience gave me a better understanding of how real projects are handled, and it made me more interested to learn about DevOps in the future.
        </p>
      </div>
    </div>
  </div>

</div>
